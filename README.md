## SpiderBeauty （仅供学习研究，勿作他用）

### 1. 简介
**拉取了一些网站的图片资源到本地，俗称爬虫，又因多是美女，故命名为Beauty**。目前共收录了8个网站，下面会简单介绍，可能不定期更新。

### 2. 环境
纯shell实现，所需命令：
- curl
- sed
- tr
- perl
- trap
- exec
- date
- echo
- cat
- wait
- read
- 等等

都是系统自带的基本命令，若无则需安装。因为没有 windows 机器，只验证了在 Mac Shell 上是可以执行的，理论上来讲，Mac Shell 里能跑的东西，在 Windows 的 Git Bash 窗口里也能跑起来。

### 3. 下载
先切换到对应的网站的 scripts 目录下，然后执行命令
```shell
$ sh get_all_images.sh 1 -1 64
```
前两个参数表示下载组图的起止角标，-1 表示下载到最后一个。第三个参数 64 表示启动的最大进程数量，多几个进程会显著提高下载速度，但不要为了快而图大，过犹不及，进程太多也没用，毕竟其他资源也是有限的，比如带宽、CPU、风扇等。闲时让 CPU 跑到 70%-80%，忙时跑到 30% 就不会影响正常使用。

### 4. 关于代理
出于某些原因，所有操作我都挂上了代理，建议下载时挂上，有些便宜的节点 Rate 只有 0.1，1GB 的流量可以下 10GB 的数据。

另外还有个原因，显而易见的是，这些网站没一个是境内的，离得最近的都是在香港，其他都在境外，用的 CloudFlare CDN 代理自己的网站，所以有些资源不挂代理可能下不下来（未验证）。

`curl`的代理有两种模式，`http`和`socks`（这里用的是`socks5`），二者都可以代理，`socks5`只是简单传递数据包，所以速度更快，但有时`SSL`验证会失败，`curl`错误码 35，这时候改成`http`代理即可，格式如下
```shell
$ curl -x http://127.0.0.1:7890
$ curl -x scoks5://127.0.0.1:7890
```

当然，如果不想挂代理，删掉脚本中`curl`命令的`-x`参数即可。

### 4. 网站墙
- [1yis](https://www.1y.is/) **77.4GB** **56万张**
每组图片都有人物介绍，看起来很正经。
- [Dopamine Girl](https://dopaminegirl.com/) **143GB** **70万张**
dopamine 是多巴胺的意思，刚查来的。这个网站的体量相当之巨大，有两万多个 article，仓库里我只上传了 4000 个，其他的若需要自己通过脚本下载。虽然数量多，但绝对不是来凑数的。
- [Ever IA](https://everia.club/) **97GB** **27万张**
这里面日本女菩萨居多，不乏平时熟知的那些，所以尺度自然也稍稍拉开了一些，广告也贼多，一张图上能盖好几层，点到第三下，甚至第四下才能点到图片。
- [Fulitu]() **659MB** **4943张**
在我下载时，还是好好的，等到我现在写文档时，网站已经 GG 了，图片不是很多，都是萝莉，轻轻惋惜一下。
- [Hefollo](https://hefollo.com/) **75GB** **9万张** **5189张**
这是一个在线网盘，存了各种图片，风景、城市、动漫、壁纸、美女，等等，4K壁纸确实很清晰。同时，作者提供了下载接口。
- [Kong Blog](https://xn--0trs0db7pba982x1yd.zhaofeiyan.cf/) **15GB** **6.5万张**
正经的写真组图，很高清，还在不断更新中。里面还提供了付费订阅，正不正经就不知道了。
- [ONS](https://ons.ooo/) **30GB** **16万张**
大同小异，几乎都是写真。
- [MM图](https://mm.tvv.tw/) **2.3GB** **1.6万张**
这个网站的滚动有些魔幻，就像抹了润滑油，摩擦阻尼在这里没有容身之地，稍微滑一点，页面就会滚动很多。这应该是个独立老哥自己维护的个人站。


### 4. 实现
如果不关心，这部分可以跳过。

这种类型的网站结构，大致如下，
```shell
一个网站有多个page，
一个page里有多个article，
一个article里是一组图片
```
所以，最终网站里的所有图片，是以其所在的article分组。如果全独立散开，单图还好，但对于组图看起来多有不便，因此 article 是不再分割的最小粒度。在`data`目录下，一个文件夹即为一组图片

因为图片的URL是不变的，所以需要保存到本地，来避免重复请求浪费流量，也减小网站的部分负载。而图片的URL又是从article中获取，有些网站article的数量是相当可观的，少则数千，多则数万，连续请求时出错几率会成倍增加，为避免重复请求和便于数据管理，所以article的URL也需要保存到本地。同理，虽然在这个对应关系中page的数量最少，但也采取保存到本地的方式。如此一来，凡请求成功的数据，均不会重复请求。

根据这个方案，大致流程如下
- 下载所有的 page，拿到所有的 article 的 URL，对应脚本 `get_all_articles.sh`
- 由 article 的URL，下载其中所有图片的 URL，对应脚本 `get_all_urls.sh`
- 根据图片的URL，下载图片，对应脚本 `get_all_imgs.sh`

**注意，因未使用绝对路径，而使用的是相对路径，所以，每个脚本均需要在其父目录`scripts`下执行。**

